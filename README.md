# ğŸ“Š Sales Forecasting Project â€“ Rossmann Store Sales

## ğŸ“Œ Project Overview

This project focuses on predicting daily sales for Rossmann stores using Machine Learning and Time Series techniques.

The objective is to help store managers forecast future sales based on historical data, promotions, holidays, competition distance, and store characteristics.

The project includes:

- Data understanding
- Data cleaning & EDA
- Feature engineering
- Machine Learning modeling (Random Forest)
- Time Series analysis
- Deep Learning (LSTM)
- Model comparison
- Deployment using Streamlit

---

## ğŸ¯ Problem Statement

Rossmann operates over 1,000 stores across Europe. Store sales are influenced by:

- Promotions
- Holidays
- Seasonality
- Store type
- Competition distance
- Assortment type

The goal is to build predictive models that estimate future sales accurately.

---

## ğŸ“‚ Dataset Description

The dataset consists of:

### 1ï¸âƒ£ train.csv
Historical daily sales data including:
- Store
- DayOfWeek
- Date
- Sales (Target)
- Customers
- Open
- Promo
- StateHoliday
- SchoolHoliday

### 2ï¸âƒ£ test.csv
Similar structure as train but without Sales.

### 3ï¸âƒ£ store.csv
Store-level information:
- StoreType
- Assortment
- CompetitionDistance
- CompetitionOpenSinceMonth
- CompetitionOpenSinceYear
- Promo2
- Promo2SinceWeek
- Promo2SinceYear
- PromoInterval

---

## ğŸ§ª Project Workflow (Notebook Structure)

### ğŸ“˜ Notebook 01 â€“ Data Understanding
- Loaded datasets
- Checked duplicates
- Checked missing values
- Merged train + store
- Merged test + store

---

### ğŸ“˜ Notebook 02 â€“ Data Cleaning & EDA
- Missing value treatment
- Outlier detection
- Univariate, Bivariate, Multivariate analysis
- Sales distribution analysis
- Correlation heatmap
- Skewness analysis
- Seasonality analysis
- Holiday effect analysis
- Promo impact analysis
- Competition distance impact
- Weekend vs weekday analysis

---

### ğŸ“˜ Notebook 03 â€“ Feature Engineering
- Date feature extraction (Year, Month, Week, Day)
- Competition duration calculation
- Promo duration calculation
- Encoding categorical variables
- Feature selection

---

### ğŸ“˜ Notebook 04 â€“ Machine Learning Modeling
Models trained:
- Baseline Model
- Linear Regression
- Random Forest Regressor

Evaluation Metrics:
- MAE (Mean Absolute Error)
- RMSE (Root Mean Squared Error)
- RÂ² Score

Random Forest performed best and was selected for deployment.

---

### ğŸ“˜ Notebook 05 â€“ Time Series Analysis
Performed:
- Stationarity check (ADF Test)
- Autocorrelation (ACF)
- Partial Autocorrelation (PACF)
- Trend and Seasonality analysis

This confirmed that past sales influence future sales.

---

### ğŸ“˜ Notebook 06 â€“ Deep Learning (LSTM)
Steps:
1. Converted data into time series format
2. Checked stationarity
3. Created sliding window sequences
4. Scaled data to (-1, 1)
5. Built 2-layer LSTM model
6. Trained model
7. Evaluated performance

Random Forest was more stable and selected for deployment.

---

## ğŸ“Š Model Comparison

| Model | MAE | RMSE | RÂ² |
|--------|------|-------|------|
| Baseline | High | High | Low |
| Linear Regression | Medium | Medium | Moderate |
| Random Forest | Lowest | Lowest | Highest |
| LSTM | Competitive | Slightly Higher | Good |

âœ… Random Forest selected as best model.

---

## ğŸš€ Deployment (Streamlit App)

The application allows:

- Input: Store ID
- Upload CSV (test.csv format)
- Automatic merging with store-level data
- Feature engineering
- Sales prediction
- Sales visualization
- Download predictions as CSV

---

## ğŸ“‚ Project Structure

```
sales_forecasting_project/
â”‚
â”œâ”€â”€ app/                          # Streamlit Deployment Application
â”‚   â”œâ”€â”€ app.py                    # Main Streamlit app
â”‚   â””â”€â”€ requirements.txt          # App dependencies
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_data/                 # Original datasets
â”‚   â”‚   â”œâ”€â”€ train.csv
â”‚   â”‚   â”œâ”€â”€ test.csv
â”‚   â”‚   â””â”€â”€ store.csv
â”‚   â”‚
â”‚   â””â”€â”€ processed_data/           # Cleaned & engineered datasets
â”‚       â”œâ”€â”€ train_merged.csv
â”‚       â”œâ”€â”€ test_merged.csv
â”‚       â”œâ”€â”€ train_features.csv
â”‚       â”œâ”€â”€ train_target.csv
â”‚       â”œâ”€â”€ X_test.npy
â”‚       â””â”€â”€ y_test.npy
â”‚
â”œâ”€â”€ models/                       # Saved trained models
â”‚   â”œâ”€â”€ random_forest_model.pkl
â”‚   â”œâ”€â”€ feature_columns.pkl
â”‚   â””â”€â”€ lstm_model.h5
â”‚
â”œâ”€â”€ notebooks/                    # Jupyter Notebooks
â”‚   â”œâ”€â”€ 01_data_understanding.ipynb
â”‚   â”œâ”€â”€ 02_data_cleaning_and_EDA.ipynb
â”‚   â”œâ”€â”€ 03_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 04_machine_learning_modeling.ipynb
â”‚   â”œâ”€â”€ 05_time_series_analysis.ipynb
â”‚   â””â”€â”€ 06_deep_learning_LSTM.ipynb
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```
---

## âš™ï¸ How to Run the App

### Step 1: Create virtual environment

```bash
python3.11 -m venv venv
source venv/bin/activate
```

### Step 2: Install Dependency
```bash
pip install -r requirements.txt
```

### Step 3: Run StreamLit
```bash
cd app
streamlit run app.py
```

## ğŸ“‚ Project Structure

The project follows a modular and organized structure separating raw data, processed data, models, notebooks, and deployment code to ensure reproducibility and scalability.

## ğŸ“‚ Google Drive
All the files and folder where uploaded in google drive because there is large file which is not uploaded in github because it doesnot have capacity for uploade so.
drive link:- 
